{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike lanes and hierarchical modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, scipy as sp, pandas as pd, pymc3 as pm, matplotlib.pyplot as plt, theano.tensor as tt\n",
    "from scipy import stats\n",
    "from modelutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycles = np.array([16,9,10,13,19,20,18,17,35,55])\n",
    "others = np.array([58,90,48,57,103,57,86,112,273,64])\n",
    "\n",
    "df = pd.DataFrame(data = np.column_stack((bicycles, others)), columns = ['bicycles', 'others'] )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonhierarchical models\n",
    "\n",
    "In Exercise 3.8, we constructed a fully pooled Bayesian model, presuming a single parameter $\\theta$ for the proportion of vehicles on the road that are bicycles. This model is quite straightforward:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_j &\\sim \\mathrm{Binomial}(\\theta, n_j) \\\\\n",
    "\\theta &\\sim \\mathrm{Beta}(\\alpha_0, \\beta_0)\n",
    "\\end{align}\n",
    "$$\n",
    "where $y_j$ is the number of bicycles, and $n_j$ the number of total vehicles observed on each street. Then, the posterior distribution is\n",
    "$$\n",
    "\\theta | y_j \\sim \\mathrm{Beta}(\\alpha_0 + y_j, \\beta_0 + (n_j - y_j) )\n",
    "$$\n",
    "We can reasonably take $\\alpha_0 = \\beta_0 = 1$ so that the prior is uniform, but we leave the code below flexible enough to make some changes in case we prefer a weakly informative prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0, b0 = 1, 1\n",
    "\n",
    "with pm.Model() as pooled_model:\n",
    "    theta = pm.Beta('theta', a0, b0)\n",
    "    y_obs = pm.Binomial('y_obs', p=theta, n=df.sum().sum(), observed = df.bicycles.sum())\n",
    "    pooled_trace = pm.sample(1000)    \n",
    "\n",
    "with pm.Model() as separate_model:\n",
    "    theta = pm.Beta('theta', a0, b0, shape = 10)\n",
    "    y_obs = pm.Binomial('y_obs', p=theta, n=df.sum(axis=1), observed = df.bicycles)\n",
    "    separate_trace = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(pooled_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(separate_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical model\n",
    "\n",
    "The nonhierarchical models of the previous section lead us to a forced dichotomy: either we estimate a single $\\theta$, or we estimate $J$ completely independent $\\theta_j$s. But neither of these approaches are fully satisfactory:\n",
    "* It is likely that different streets are in fact different, so we shouldn't expect them all to be equivalent, and pooling the observations to estimate a single $\\theta$ doesn't yield the right estimate.\n",
    "* On the other hand, it is unreasonable to transfer no information from one observation to another. Streets in the same neighborhood likely obey at least somewhat similar patterns of use.\n",
    "\n",
    "The Bayesian approach allows us a compromise: so-called \"partial pooling\" with a hierarchical (aka multi-level) model. In a hierarchical model, we consider each of the $\\theta_j$s to be independently drawn from a distribution $p(\\theta | \\phi)$, conditional on so-called *hyperparameters* $\\phi$, which we estimate along with the $\\theta_j$s. This can be rendered as a DAG -- in many cases, a tree -- with the hyperparameters at the top, and parameters below it. (There is no need for a limitation on the number of levels in the model, but the simplest versions, which we'll start with, have only two levels.)\n",
    "\n",
    "We'll stick with a binomial likelihood and with beta distributions for the $\\theta_j$, but instead of using a fixed prior beta distribution, we take the priors for $\\theta_j$ to be $\\mathrm{Beta}(\\alpha, \\beta)$, with $\\alpha, \\beta$ themselves drawn from a *hyperprior* distribution $p(\\alpha, \\beta)$.\n",
    "\n",
    "Our model is now:\n",
    "$$\n",
    "\\begin{align}\n",
    "y_j &\\sim \\mathrm{Binomial}(\\theta_j, n_j) \\\\\n",
    "\\theta_j &\\sim \\mathrm{Beta}(\\alpha, \\beta) \\\\\n",
    "p(\\alpha, \\beta) &\\propto (\\alpha + \\beta)^{-5/2}\n",
    "\\end{align}\n",
    "$$\n",
    "Discussion of the hyperprior $p(\\alpha, \\beta) \\propto (\\alpha + \\beta)^{-5/2}$ in lecture or BDA section 5.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hard way\n",
    "\n",
    "The \"hard way\" to do inference on this model is to explicitly compute the posterior density for $\\alpha, \\beta$ and use it to sample from the joint posterior distribution of $(\\alpha, \\beta, \\theta_1, \\ldots, \\theta_{10})$. We go through this exercise for the sake of completeness.\n",
    "\n",
    "#### Step 1: calculate the posterior density for $\\alpha$, $\\beta$\n",
    "\n",
    "A formula for the posterior density $p(\\alpha, \\beta | y)$ for a similar example can be found in section 5.3 of the book:\n",
    "\n",
    "$$ p(\\alpha, \\beta | y) = p(\\alpha, \\beta) \\prod_{j=1}^J \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\frac{\\Gamma(\\alpha + y_j) \\Gamma(\\beta + n_j - y_j)}{\\Gamma(\\alpha + \\beta + n_j)} $$\n",
    "\n",
    "This allows us to compute $p(\\alpha, \\beta | y)$ on a grid of $\\alpha, \\beta$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the un-normalized posterior density for alpha and beta and plot contours.\n",
    "'''\n",
    "\n",
    "from scipy.special import gammaln # log of the gamma function\n",
    "\n",
    "# Work in logs to prevent underflows.\n",
    "# The posterior density can be found in section 5.3 of BDA (equation 5.8)\n",
    "def logposterior(a, b, data):\n",
    "    post = (-5/2) * np.log(a + b)\n",
    "    for j in range(len(data)):\n",
    "        post += gammaln(a + b) - gammaln(a) - gammaln(b)\n",
    "        post += gammaln(a + data.bicycles[j]) + gammaln(b + data.others[j]) - gammaln(a + b + data.bicycles[j] + data.others[j])\n",
    "    return post\n",
    "\n",
    "alphagrid = np.linspace(0.01, 8, 500)\n",
    "betagrid = np.linspace(0.01, 25, 500)\n",
    "\n",
    "X, Y = np.meshgrid(alphagrid, betagrid)\n",
    "Z = logposterior(X, Y, df)\n",
    "Z -= np.max(Z)\n",
    "Z = np.exp(Z)\n",
    "fig, ax = plt.subplots()\n",
    "CS = ax.contour(X, Y, Z)\n",
    "#ax.clabel(CS, inline=1, fontsize=10)\n",
    "ax.set_title('Contours of posterior distribution for $\\\\alpha$, $\\\\beta$')\n",
    "ax.set_xlabel('$\\\\alpha$')\n",
    "ax.set_ylabel('$\\\\beta$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate a normalized posterior density\n",
    "\n",
    "alphagrid = np.arange(0.01, 20.01, 0.02)\n",
    "betagrid = np.arange(0.01, 40.01, 0.02)\n",
    "X, Y = np.meshgrid(alphagrid, betagrid)\n",
    "Z = logposterior(X, Y, df)\n",
    "Z -= np.max(Z)\n",
    "Z = np.exp(Z)\n",
    "normconst = np.sum(Z) * (0.02 ** 2) # Don't forget to scale by the grid size!\n",
    "Z /= normconst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a joint PDF for $\\alpha, \\beta$, we can use this to sample from the posterior by the following approach:\n",
    "1. Find the marginal density of $\\alpha$ by summing the joint PDF over $\\beta$.\n",
    "2. Draw samples of $\\alpha$ from this marginal density.\n",
    "3. For each sample value of $\\alpha$, get the conditional density of $\\beta$ by taking a slice from the joint PDF, and use this to draw a sample value of $\\beta$.\n",
    "4. For each sampled pair $(\\alpha_i, \\beta_i)$, draw a vector of $\\theta_j$s from their posterior distribution $\\theta_j \\sim \\mathrm{Beta}(\\alpha_i + y_j, \\beta_i + n_j - y_j)$.\n",
    "\n",
    "Then we can examine our sampled $\\theta_j$s to make inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginal posterior of alpha\n",
    "\n",
    "margina = np.sum(Z, 0)\n",
    "margina /= np.sum(margina * 0.02)\n",
    "plt.plot(alphagrid, margina)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample from this distribution, we use the inverse-CDF method. Recall the CDF of a random variable is the function $F(x) = \\mathrm{Pr}(X \\leq x)$. If we know the inverse CDF $F^{-1}(p)$, we can feed it values uniformly drawn from $[0, 1]$ and obtain samples from the distribution of $X$. Below, we define a simple function to implement the inverse CDF method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having estimated the pdf of alpha, we can draw samples from this distribution by the inverse-cdf method\n",
    "\n",
    "def inverse_cdf(p, grid, pdf_array):\n",
    "    '''\n",
    "    Computes the inverse CDF of a probability given an estimated PDF evaluated on a grid.\n",
    "    Parameters:    \n",
    "    '''\n",
    "    spacing = grid[1] - grid[0]\n",
    "    totp = 0\n",
    "    for i in range(len(grid)):\n",
    "        if p < totp:\n",
    "            return grid[i]\n",
    "        totp += pdf_array[i] * spacing\n",
    "    return grid[-1]\n",
    "\n",
    "alpha_sample = []\n",
    "for i in range(2000):\n",
    "    x = np.random.rand()\n",
    "    alpha_sample.append(inverse_cdf(x, alphagrid, margina))\n",
    "alpha_sample = np.array(alpha_sample)\n",
    "np.average(alpha_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_sample = []\n",
    "for alpha in alpha_sample:\n",
    "    idx = np.where(alphagrid == alpha)\n",
    "    marginb = Z[:, idx[0][0]]\n",
    "    marginb /= np.sum(marginb) * 0.02\n",
    "    x = np.random.rand()\n",
    "    beta_sample.append(inverse_cdf(x, betagrid, marginb))\n",
    "beta_sample = np.array(beta_sample)\n",
    "np.average(beta_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can sample some thetas\n",
    "theta = np.zeros((2000, 10))\n",
    "for i in range(2000):\n",
    "    for j in range(10):\n",
    "        theta[i,j] = sp.stats.beta.rvs(alpha_sample[i] + df.bicycles[j], beta_sample[i] + df.others[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all the posteriors for thetas\n",
    "for i in range(10):\n",
    "    kernel = sp.stats.gaussian_kde(theta[:, i])\n",
    "    grid = np.arange(0, 1.005, .005)\n",
    "    plt.plot(grid, kernel(grid))\n",
    "plt.show()\n",
    "\n",
    "# Posterior means\n",
    "np.average(theta, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The easy way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp_ab(value):\n",
    "    '''Transformation for the hyperprior. Theano tensor magic.'''\n",
    "    return tt.log(tt.pow(tt.sum(value), -5/2))\n",
    "\n",
    "with pm.Model() as hierarchical_model:\n",
    "    # Uninformative prior for alpha and beta\n",
    "    ab = pm.HalfFlat('ab',\n",
    "                     shape=2,\n",
    "                     testval=np.asarray([1., 1.]))\n",
    "    pm.Potential('p(a, b)', logp_ab(ab))\n",
    "\n",
    "    # Distributions for theta\n",
    "    theta = pm.Beta('theta', alpha=ab[0], beta=ab[1], shape = 10)\n",
    "    \n",
    "    # Data distribution\n",
    "    y_obs = pm.Binomial('y_obs', p = theta, observed = df.bicycles, n=df.sum(axis=1))\n",
    "    \n",
    "    trace = pm.sample(2000, target_accept = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    kernel = sp.stats.gaussian_kde(trace['theta'][:, i])\n",
    "    grid = np.arange(0, 1.01, .01)\n",
    "    plt.plot(grid, kernel(grid))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_summary = pm.summary(separate_trace)\n",
    "hier_summary = pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(10), separate_summary['mean'], 'o', mfc='none', label = 'Separate effects model')\n",
    "plt.plot(range(10), hier_summary[2:]['mean'], 'o', label = 'Hierarchical model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
