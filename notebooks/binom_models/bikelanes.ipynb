{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike lanes and hierarchical modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, scipy as sp, pandas as pd, pymc3 as pm, matplotlib.pyplot as plt, theano.tensor as tt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from Table 3.3 in BDA\n",
    "bicycles = np.array([16,9,10,13,19,20,18,17,35,55])\n",
    "others = np.array([58,90,48,57,103,57,86,112,273,64])\n",
    "\n",
    "df = pd.DataFrame(data = np.column_stack((bicycles, others)), columns = ['bicycles', 'others'] )\n",
    "df['total'] = df['bicycles'] + df['others']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonhierarchical models\n",
    "\n",
    "Two nonhierarchical models: \n",
    "* a fully pooled model, where we treat each street as a separate observation of a single underlying parameter\n",
    "* a fully separated model, where we treat each street as an independent quantity with its own parameter\n",
    "\n",
    "#### Pooled\n",
    "$$\n",
    "\\begin{align}\n",
    "y_j &\\sim \\mathrm{Binomial}(\\theta, n_j) \\\\\n",
    "\\theta &\\sim \\mathrm{Beta}(\\alpha_0, \\beta_0)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $y_j$ is the number of bicycles, and $n_j$ the number of total vehicles observed on each street. Then, the posterior distribution is\n",
    "\n",
    "$$\n",
    "\\theta | y_j \\sim \\mathrm{Beta}(\\alpha_0 + \\sum_j y_j, \\beta_0 + \\sum_j (n_j - y_j) )\n",
    "$$\n",
    "\n",
    "#### Separated\n",
    "$$\n",
    "\\begin{align}\n",
    "y_j &\\sim \\mathrm{Binomial}(\\theta_j, n_j) \\\\\n",
    "\\theta_j &\\sim \\mathrm{Beta}(\\alpha_0, \\beta_0)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $y_j$ is the number of bicycles, and $n_j$ the number of total vehicles observed on each street. Then, the posterior distribution is\n",
    "\n",
    "$$\n",
    "\\theta_j | y_j \\sim \\mathrm{Beta}(\\alpha_0 + y_j, \\beta_0 + (n_j - y_j) )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0, b0 = 1, 3\n",
    "\n",
    "### Pooled model\n",
    "\n",
    "pooled_posterior = sp.stats.beta(a0 + df['bicycles'].sum(), b0 + df['others'].sum())\n",
    "\n",
    "### Separated model\n",
    "\n",
    "separated_posteriors = [sp.stats.beta(a0 + df.loc[i, 'bicycles'], b0 + df.loc[i, 'others']) for i in range(len(df))]\n",
    "len(separated_posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.linspace(0, 1, 1000)\n",
    "ymax = pooled_posterior.pdf(pooled_posterior.mean())\n",
    "\n",
    "plt.figure(figsize = (12, 9))\n",
    "plt.plot(grid, pooled_posterior.pdf(grid), 'b-', label = 'Posterior density')\n",
    "plt.vlines(x=(pooled_posterior.ppf(0.045), pooled_posterior.ppf(0.955)), ymin=0, ymax=ymax, color='g', linestyle='--', label = '91% interval')\n",
    "plt.xlabel('Value of $\\\\theta$')\n",
    "plt.ylabel('Probability density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['b','g','r','y','orange','k','c','m','gray', 'darkgreen']\n",
    "\n",
    "plt.figure(figsize = (12, 9))\n",
    "for i in range(len(separated_posteriors)):\n",
    "    plt.plot(grid, separated_posteriors[i].pdf(grid), label = 'Posterior density for street ' + str(i), color = colors[i])\n",
    "plt.xlabel('Value of $\\\\theta$')\n",
    "plt.ylabel('Probability density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(separated_posteriors[i].ppf(0.045), separated_posteriors[i].ppf(0.955)) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical model\n",
    "\n",
    "The nonhierarchical models of the previous section lead us to a forced dichotomy: either we estimate a single $\\theta$, or we estimate $J$ completely independent $\\theta_j$s. But neither of these approaches are fully satisfactory:\n",
    "* It is likely that different streets are in fact different, so we shouldn't expect them all to be equivalent, and pooling the observations to estimate a single $\\theta$ doesn't yield the right estimate.\n",
    "* On the other hand, it is unreasonable to transfer no information from one observation to another. Streets in the same neighborhood likely obey at least somewhat similar patterns of use.\n",
    "\n",
    "The Bayesian approach allows us a compromise: so-called \"partial pooling\" with a hierarchical (aka multi-level) model. In a hierarchical model, we consider each of the $\\theta_j$s to be independently drawn from a distribution $p(\\theta | \\phi)$, conditional on so-called *hyperparameters* $\\phi$, which we estimate along with the $\\theta_j$s. This can be rendered as a DAG -- in many cases, a tree -- with the hyperparameters at the top, and parameters below it. (There is no need for a limitation on the number of levels in the model, but the simplest versions, which we'll start with, have only two levels.)\n",
    "\n",
    "We'll stick with a binomial likelihood and with beta distributions for the $\\theta_j$, but instead of using a fixed prior beta distribution, we take the priors for $\\theta_j$ to be $\\mathrm{Beta}(\\alpha, \\beta)$, with $\\alpha, \\beta$ themselves drawn from a *hyperprior* distribution $p(\\alpha, \\beta)$.\n",
    "\n",
    "Our model is now:\n",
    "$$\n",
    "\\begin{align}\n",
    "y_j &\\sim \\mathrm{Binomial}(\\theta_j, n_j) \\\\\n",
    "\\theta_j &\\sim \\mathrm{Beta}(\\alpha, \\beta) \\\\\n",
    "\\alpha &:= \\eta \\mu \\\\\n",
    "\\beta &:= \\eta(1-\\mu) \\\\\n",
    "\\mu &\\sim \\mathrm{Beta}(1, 3) \\\\\n",
    "\\eta &\\sim \\mathrm{HalfCauchy}(1)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here, $\\mu, \\eta$ are location and scale parameters for the distribution from which $\\theta_j$ are drawn. We assign weakly informative priors to these and derive $\\alpha, \\beta$ from them. Notice we don't have to explicitly specify priors for $\\alpha$ and $\\beta$ (but we could -- see below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model in PyMC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hierarchical_model:\n",
    "    # Weakly informative prior for alpha and beta\n",
    "    mu = pm.Beta('mu', 1, 3)\n",
    "    eta = pm.HalfCauchy('eta', 1)\n",
    "    \n",
    "    alpha = pm.Deterministic('alpha', eta * mu)\n",
    "    beta = pm.Deterministic('beta', eta * (1 - mu))\n",
    "    \n",
    "    # Distributions for theta\n",
    "    theta = pm.Beta('theta', alpha=alpha, beta=beta, shape = 10) # shape = 10 makes a vector of 10 parameters\n",
    "    \n",
    "    # Data distribution\n",
    "    y_obs = pm.Binomial('y_obs', p = theta, observed = df.bicycles, n=df.total)\n",
    "    \n",
    "    # Inference\n",
    "    trace = pm.sample(4000, target_accept = 0.8, tune = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pm.summary(trace)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two useful visualizations:\n",
    "\n",
    "* Forest plot: lets us compare the individual streets\n",
    "* Posterior plot: we've seen this before, but it's most useful for examining the hyperparameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(trace, var_names = ['theta'])\n",
    "pm.plot_posterior(trace, var_names = ['mu', 'eta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 9))\n",
    "for i in range(10):\n",
    "    kernel = sp.stats.gaussian_kde(trace['theta'][:, i])\n",
    "    plt.plot(grid, kernel(grid), label = 'Density estimate for street ' + str(i), color = colors[i])\n",
    "    #plt.plot(grid, separated_posteriors[i].pdf(grid), label = 'Posterior density for street ' + str(i))\n",
    "plt.xlabel('Value of $\\\\theta$')\n",
    "plt.ylabel('Probability density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(trace['mu'], trace['eta'], 'b.', alpha = 0.2)\n",
    "plt.xlabel('Posterior $\\\\mu$')\n",
    "plt.ylabel('Posterior $\\\\eta$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(trace['alpha'], trace['beta'], 'b.', alpha = 0.2)\n",
    "plt.xlabel('Posterior $\\\\mu$')\n",
    "plt.ylabel('Posterior $\\\\eta$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 9))\n",
    "for i in range(10):\n",
    "    kernel = sp.stats.gaussian_kde(trace['theta'][:, i])\n",
    "    plt.plot(grid, kernel(grid), label = 'Density estimate for street ' + str(i), color = colors[i])\n",
    "    plt.plot(grid, separated_posteriors[i].pdf(grid), linestyle='--', color = colors[i], alpha = 0.5)\n",
    "plt.xlabel('Value of $\\\\theta$')\n",
    "plt.ylabel('Probability density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(range(10), [separated_posteriors[i].mean() for i in range(10)], 'o', mfc='none', label = 'Separate effects model')\n",
    "plt.plot(range(10), summary[4:]['mean'], 'o', label = 'Hierarchical model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Suppose we go back to street 5 and count vehicles until we've observed 100 total. Simulate 1000 runs from the predictive distribution for the number of bicycles, and find a 90% interval for this count. *(Hint: take 1000 samples from the trace for that street's theta parameter, and use them to generate binomial counts for 100 trials.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = trace['theta'][:1000,5]\n",
    "counts = sp.stats.binom.rvs(100, thetas)\n",
    "(np.quantile(counts, 0.05), np.quantile(counts, 0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Suppose we select a new street from a similar \"population\" of streets and count vehicles until we've observed 100 total. Simulate 1000 runs from the predictive distribution for the number of bicycles, and find a 90% interval for this count. *(Hint: take 1000 samples from the trace for alpha and beta; use those to generate 1000 thetas; then, generate binomial counts.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The prior from BDA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp_ab(value):\n",
    "    '''Transformation for the hyperprior. Theano tensor magic.'''\n",
    "    return tt.log(tt.pow(tt.sum(value), -5/2))\n",
    "\n",
    "with pm.Model() as hierarchical_model:\n",
    "    # Uninformative prior for alpha and beta\n",
    "    ab = pm.HalfFlat('ab',\n",
    "                     shape=2,\n",
    "                     testval=np.asarray([1., 1.]))\n",
    "    pm.Potential('p(a, b)', logp_ab(ab))\n",
    "    \n",
    "    M = pm.Deterministic('M', ab[0] / tt.sum(ab))\n",
    "\n",
    "    # Distributions for theta\n",
    "    theta = pm.Beta('theta', alpha=ab[0], beta=ab[1], shape = 10)\n",
    "    \n",
    "    # Data distribution\n",
    "    y_obs = pm.Binomial('y_obs', p = theta, observed = df.bicycles, n=df.total)\n",
    "    \n",
    "    trace = pm.sample(4000, target_accept = 0.8, tune = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    kernel = sp.stats.gaussian_kde(trace['theta'][:, i])\n",
    "    grid = np.arange(0, 1.01, .01)\n",
    "    plt.plot(grid, kernel(grid))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hard way\n",
    "\n",
    "The \"hard way\" to do inference on this model is to explicitly compute the posterior density for $\\alpha, \\beta$ and use it to sample from the joint posterior distribution of $(\\alpha, \\beta, \\theta_1, \\ldots, \\theta_{10})$. We go through this exercise for the sake of completeness.\n",
    "\n",
    "#### Step 1: calculate the posterior density for $\\alpha$, $\\beta$\n",
    "\n",
    "A formula for the posterior density $p(\\alpha, \\beta | y)$ for a similar example can be found in section 5.3 of the book:\n",
    "\n",
    "$$ p(\\alpha, \\beta | y) = p(\\alpha, \\beta) \\prod_{j=1}^J \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\frac{\\Gamma(\\alpha + y_j) \\Gamma(\\beta + n_j - y_j)}{\\Gamma(\\alpha + \\beta + n_j)} $$\n",
    "\n",
    "This allows us to compute $p(\\alpha, \\beta | y)$ on a grid of $\\alpha, \\beta$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the un-normalized posterior density for alpha and beta and plot contours.\n",
    "'''\n",
    "\n",
    "from scipy.special import gammaln # log of the gamma function\n",
    "\n",
    "# Work in logs to prevent underflows.\n",
    "# The posterior density can be found in section 5.3 of BDA (equation 5.8)\n",
    "def logposterior(a, b, data):\n",
    "    post = (-5/2) * np.log(a + b)\n",
    "    for j in range(len(data)):\n",
    "        post += gammaln(a + b) - gammaln(a) - gammaln(b)\n",
    "        post += gammaln(a + data.bicycles[j]) + gammaln(b + data.others[j]) - gammaln(a + b + data.bicycles[j] + data.others[j])\n",
    "    return post\n",
    "\n",
    "alphagrid = np.linspace(0.01, 8, 500)\n",
    "betagrid = np.linspace(0.01, 25, 500)\n",
    "\n",
    "X, Y = np.meshgrid(alphagrid, betagrid)\n",
    "Z = logposterior(X, Y, df)\n",
    "Z -= np.max(Z)\n",
    "Z = np.exp(Z)\n",
    "fig, ax = plt.subplots()\n",
    "CS = ax.contour(X, Y, Z)\n",
    "#ax.clabel(CS, inline=1, fontsize=10)\n",
    "ax.set_title('Contours of posterior distribution for $\\\\alpha$, $\\\\beta$')\n",
    "ax.set_xlabel('$\\\\alpha$')\n",
    "ax.set_ylabel('$\\\\beta$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate a normalized posterior density\n",
    "\n",
    "alphagrid = np.arange(0.01, 20.01, 0.02)\n",
    "betagrid = np.arange(0.01, 40.01, 0.02)\n",
    "X, Y = np.meshgrid(alphagrid, betagrid)\n",
    "Z = logposterior(X, Y, df)\n",
    "Z -= np.max(Z)\n",
    "Z = np.exp(Z)\n",
    "normconst = np.sum(Z) * (0.02 ** 2) # Don't forget to scale by the grid size!\n",
    "Z /= normconst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a joint PDF for $\\alpha, \\beta$, we can use this to sample from the posterior by the following approach:\n",
    "1. Find the marginal density of $\\alpha$ by summing the joint PDF over $\\beta$.\n",
    "2. Draw samples of $\\alpha$ from this marginal density.\n",
    "3. For each sample value of $\\alpha$, get the conditional density of $\\beta$ by taking a slice from the joint PDF, and use this to draw a sample value of $\\beta$.\n",
    "4. For each sampled pair $(\\alpha_i, \\beta_i)$, draw a vector of $\\theta_j$s from their posterior distribution $\\theta_j \\sim \\mathrm{Beta}(\\alpha_i + y_j, \\beta_i + n_j - y_j)$.\n",
    "\n",
    "Then we can examine our sampled $\\theta_j$s to make inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginal posterior of alpha\n",
    "\n",
    "margina = np.sum(Z, 0)\n",
    "margina /= np.sum(margina * 0.02)\n",
    "plt.plot(alphagrid, margina)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample from this distribution, we use the inverse-CDF method. Recall the CDF of a random variable is the function $F(x) = \\mathrm{Pr}(X \\leq x)$. If we know the inverse CDF $F^{-1}(p)$, we can feed it values uniformly drawn from $[0, 1]$ and obtain samples from the distribution of $X$. Below, we define a simple function to implement the inverse CDF method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having estimated the pdf of alpha, we can draw samples from this distribution by the inverse-cdf method\n",
    "\n",
    "def inverse_cdf(p, grid, pdf_array):\n",
    "    '''\n",
    "    Computes the inverse CDF of a probability given an estimated PDF evaluated on a grid.\n",
    "    Parameters:    \n",
    "    '''\n",
    "    spacing = grid[1] - grid[0]\n",
    "    totp = 0\n",
    "    for i in range(len(grid)):\n",
    "        if p < totp:\n",
    "            return grid[i]\n",
    "        totp += pdf_array[i] * spacing\n",
    "    return grid[-1]\n",
    "\n",
    "alpha_sample = []\n",
    "for i in range(2000):\n",
    "    x = np.random.rand()\n",
    "    alpha_sample.append(inverse_cdf(x, alphagrid, margina))\n",
    "alpha_sample = np.array(alpha_sample)\n",
    "np.average(alpha_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_sample = []\n",
    "for alpha in alpha_sample:\n",
    "    idx = np.where(alphagrid == alpha)\n",
    "    marginb = Z[:, idx[0][0]]\n",
    "    marginb /= np.sum(marginb) * 0.02\n",
    "    x = np.random.rand()\n",
    "    beta_sample.append(inverse_cdf(x, betagrid, marginb))\n",
    "beta_sample = np.array(beta_sample)\n",
    "np.average(beta_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can sample some thetas\n",
    "theta = np.zeros((2000, 10))\n",
    "for i in range(2000):\n",
    "    for j in range(10):\n",
    "        theta[i,j] = sp.stats.beta.rvs(alpha_sample[i] + df.bicycles[j], beta_sample[i] + df.others[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all the posteriors for thetas\n",
    "for i in range(10):\n",
    "    kernel = sp.stats.gaussian_kde(theta[:, i])\n",
    "    grid = np.arange(0, 1.005, .005)\n",
    "    plt.plot(grid, kernel(grid))\n",
    "plt.show()\n",
    "\n",
    "# Posterior means\n",
    "np.average(theta, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
